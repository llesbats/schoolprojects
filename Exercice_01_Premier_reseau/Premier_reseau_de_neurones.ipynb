{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYa1_-AngKvS"
   },
   "source": [
    "# Votre premier réseau de neurones\n",
    "\n",
    "Dans ce projet, vous allez construire votre premier réseau neuronal.\n",
    "En utilisant le jeu de données et vous l'utiliserez pour prédire le nombre de personnes qui louent des vélos chaque jour.\n",
    "\n",
    "Nous avons fourni une partie du code, mais nous vous avons laissé le soin de mettre en œuvre le réseau neuronal (pour l'essentiel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "tA9kN3UjgKvU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtFudVo5inbA"
   },
   "outputs": [],
   "source": [
    "# A executer si code exécuté dans Collab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nIp7MKJ0gKvY"
   },
   "source": [
    "#◢ Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frSe4-p3gKvZ"
   },
   "outputs": [],
   "source": [
    "exercice1_path = 'drive/My Drive/DeepLearning/Exercice_01_Premier_reseau'\n",
    "data_path = exercice1_path + '/Bike-Sharing-Dataset/hour.csv'\n",
    "\n",
    "rides = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xvfQ3tsmgKvc"
   },
   "outputs": [],
   "source": [
    "rides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-taE8X1gKvg"
   },
   "source": [
    "#◢ Préparation des données\n",
    "\n",
    "Une étape cruciale dans le travail en machine learning est la préparation des données. \n",
    "\n",
    "Ce jeu de données contient le nombre total de vélos loués pour chaque heure de chaque jour du 1er Janvier 2011 au 31 Décembre 2012. Il contient également le nombre de vélos loués par les abonnés et les utilisateurs occasionnels du système.\n",
    "\n",
    "\n",
    "Le fichier hour.csv contient les informations suivantes :\n",
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday or not (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "- weekday : day of the week\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "Le graphique ci-dessous montre le nombre de cyclistes au cours des 10 premiers jours environ de l'ensemble des données. (Certaines données sont manquantes et certains jours n'ont pas exactement 24 entrées dans le jeu de données, ce n'est donc pas exactement 10 jours). \n",
    "\n",
    "Vous pouvez voir les locations réparties par heure ici. \n",
    "\n",
    "Ces données sont assez compliquées ! \n",
    "\n",
    "Les week-ends ont des taux de fréquentation plus bas que les autres jours et il y a des pics lorsque les gens vont et viennent à vélo au travail pendant la semaine. \n",
    "En examinant les données ci-dessus, nous avons également des informations sur la température, l'humidité et la vitesse du vent, qui ont toutes une incidence sur le nombre de vélos loués. \n",
    "\n",
    "Vous allez essayer de saisir tout cela avec votre modèle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9mfIREQgKvh"
   },
   "outputs": [],
   "source": [
    "rides[:24*10].plot(x='dteday', y='cnt', figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOnhBPgagKvj"
   },
   "source": [
    "## Dummification des variables\n",
    "\n",
    "Le jeu de données contient quelques variables catégorielles comme la saison, le temps, le mois.\n",
    "\n",
    "Pour les inclure dans notre modèle, nous devons les convertir en variables fictives binaires. \n",
    "\n",
    "C'est simple à faire avec la librairie pandas grâce à `get_dummies()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LJ6iogpVgKvk"
   },
   "outputs": [],
   "source": [
    "dummy_fields = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
    "for each in dummy_fields:\n",
    "    dummies = pd.get_dummies(rides[each], prefix=each, drop_first=False)\n",
    "    rides = pd.concat([rides, dummies], axis=1)\n",
    "\n",
    "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
    "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
    "data = rides.drop(fields_to_drop, axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "frkvk_64gKvn"
   },
   "source": [
    "## Standardisation des variables cibles\n",
    "Pour faciliter l'apprentissage du réseau de neurones et la création d'un modèle performant, nous allons standardiser chacune des variables continues. Autrement dit, nous allons déplacer et mettre à l'échelle les variables de telle sorte qu'elles aient une moyenne nulle et un écart-type de 1.\n",
    "\n",
    "Les facteurs d'échelle sont enregistrés afin que nous puissions revenir en arrière lorsque nous utiliserons le réseau pour les prédictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TvQ7bqSFgKvo"
   },
   "outputs": [],
   "source": [
    "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
    "# Store scalings in a dictionary so we can convert back later\n",
    "scaled_features = {}\n",
    "for each in quant_features:\n",
    "    mean, std = data[each].mean(), data[each].std()\n",
    "    scaled_features[each] = [mean, std]\n",
    "    data.loc[:, each] = (data[each] - mean)/std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StPw6q6JgKvr"
   },
   "source": [
    "## Diviser les données en 3 jeux de données : entrainement, validation et test\n",
    "\n",
    "Les données des 21 derniers jours (environ) seront utilisées pour le jeu de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eF3JYb50gKvs"
   },
   "outputs": [],
   "source": [
    "# Save data for approximately the last 21 days \n",
    "test_data = data[-21*24:]\n",
    "\n",
    "# Now remove the test data from the data set \n",
    "data = data[:-21*24]\n",
    "\n",
    "# Separate the data into features and targets\n",
    "target_fields = ['cnt', 'casual', 'registered']\n",
    "features, targets = data.drop(target_fields, axis=1), data[target_fields]\n",
    "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD8PN7EagKvu"
   },
   "source": [
    "Comme il s'agit de données chronologiques (time series), nous séparons le jeu de données en deux parties en gardant la notion temporelle.\n",
    "Les données les plus anciennes constitueront le jeu d'entrainement et les 60 derniers jours constitueront le jeu de données de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJWXIeSogKvv"
   },
   "outputs": [],
   "source": [
    "# Hold out the last 60 days or so of the remaining data as a validation set\n",
    "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
    "val_features, val_targets = features[-60*24:], targets[-60*24:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C40dwBb9gKvy"
   },
   "source": [
    "#◢ Construction du réseau\n",
    "\n",
    "Ci-dessous, vous trouverez la structure du code et les tests unitaires permettant de vérifier que votre implémentation est correcte.\n",
    "\n",
    "Vous allez mettre en place la propagation avant et arrière du réseau. \n",
    "Vous définirez également les hyperparamètres : le taux d'apprentissage, le nombre de neurones cachés et le nombre de fois où l'algorithme parcourira l'intégralité du jeu de données d'entrainement.\n",
    "\n",
    "<img class=\"tfo-display-only-on-site\" src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/1147px-Artificial_neural_network.svg.png\" height=\"300\"/>\n",
    "\n",
    "Le réseau comporte deux couches, une couche cachée et une couche de sortie. \n",
    "La couche cachée utilisera la fonction sigmoïde pour les activations. \n",
    "La couche de sortie n'a qu'un seul neurone et est utilisée pour la régression, la sortie du neurone est la même que l'entrée du neurone. C'est-à-dire que la fonction d'activation est $f(x)=x$. \n",
    "\n",
    "Nous utilisons les poids pour propager les signaux vers l'avant des couches d'entrée aux couches de sortie du réseau de neurones. Nous utilisons également les poids pour propager l'erreur en arrière de la sortie vers le réseau afin de mettre à jour nos poids. C'est ce qu'on appelle la *propagation arrière* ou *back propagation*.\n",
    "\n",
    "> **ASTUCE** Vous aurez besoin de la dérivée de la fonction d'activation de la sortie ($f(x) = x$) pour implémentater la backpropagation. Si vous n'êtes pas familier avec le calcul, cette fonction est équivalente à l'équation $y = x$. Quelle est la pente de cette équation ? C'est la dérivée de $f(x)$.\n",
    "\n",
    "Ci-dessous, les taches à implémenter :\n",
    "1. Implémenter la fonction sigmoïde à utiliser comme fonction d'activation. Définissez `self.activation_function` dans `__init__`.\n",
    "2. Implémenter la propagation avant dans la méthode `forwardPass`.\n",
    "3. Implémenter le calcul de l'erreur de sortie dans la méthode `train`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSznPrbvgKv1"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # Set number of nodes in input, hidden and output layers.\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        # Initialize weights\n",
    "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
    "                                       (self.input_nodes, self.hidden_nodes))\n",
    "\n",
    "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
    "                                       (self.hidden_nodes, self.output_nodes))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "        #### TODO: Set self.activation_function to your implemented sigmoid function ####\n",
    "        #\n",
    "        # Note: in Python, you can define a function with a lambda expression,\n",
    "        # as shown below.\n",
    "        self.activation_function = 0  # Replace 0 with your sigmoid calculation.\n",
    "        \n",
    "        ### If the lambda code above is not something you're familiar with,\n",
    "        # You can uncomment out the following three lines and put your \n",
    "        # implementation there instead.\n",
    "        #\n",
    "        #def sigmoid(x):\n",
    "        #    return 0  # Replace 0 with your sigmoid calculation here\n",
    "        #self.activation_function = sigmoid\n",
    "\n",
    "\n",
    "    def forwardPass(self, X):\n",
    "        ''' Run a forward pass through the network with input features \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            features: 1D array of feature values\n",
    "        '''\n",
    "        #### Implement the forward pass here ####\n",
    "        # TODO: Hidden layer - replace these values with the appropriate calculations.\n",
    "        hidden_inputs = XXX # signals into hidden layer\n",
    "        hidden_outputs = XXX # signals from hidden layer\n",
    "        \n",
    "        # TODO: Output layer - Replace these values with the appropriate calculations.\n",
    "        final_inputs = XXX # signals into final output layer\n",
    "        final_outputs = XXX # signals from final output layer \n",
    "        \n",
    "        return hidden_outputs, final_outputs\n",
    "\n",
    "    def backwardPass(self, X, delta_weights_i_h, delta_weights_h_o, hidden_outputs, error):\n",
    "        ''' Run a backward pass through the network from error output\n",
    "\n",
    "            https://en.wikipedia.org/wiki/Delta_rule\n",
    "            \n",
    "            Arguments\n",
    "            ---------\n",
    "            X: inputs\n",
    "            delta_weights_i_h:\n",
    "            delta_weights_h_o:\n",
    "            hidden_outputs:\n",
    "            error:\n",
    "        '''\n",
    "        # Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = error * self.weights_hidden_to_output.T\n",
    "        \n",
    "        # Backpropagated error terms\n",
    "        # Gradient =  Learning Rate * Erreur * Dérivée de la sortie\n",
    "        output_error_term = self.lr * error\n",
    "        hidden_error_term = self.lr * hidden_error * hidden_outputs * (1.0 - hidden_outputs)\n",
    "\n",
    "        #  DeltaPoids = Gradient (Produit matriciel) Input\n",
    "        # Weight step (input to hidden)\n",
    "        delta_weights_i_h += np.dot(X[:,None],hidden_error_term)\n",
    "        # Weight step (hidden to output)\n",
    "        delta_weights_h_o += np.dot(hidden_outputs[:,None],output_error_term[:,None])\n",
    "\n",
    "        return delta_weights_i_h, delta_weights_h_o\n",
    "    \n",
    "    def train(self, features, targets):\n",
    "        ''' Train the network on batch of features and targets. \n",
    "        \n",
    "            Arguments\n",
    "            ---------\n",
    "            \n",
    "            features: 2D array, each row is one data record, each column is a feature\n",
    "            targets: 1D array of target values\n",
    "        \n",
    "        '''\n",
    "        n_records = features.shape[0]\n",
    "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
    "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
    "        for X, y in zip(features, targets):\n",
    "            \n",
    "            ### Forward pass ###\n",
    "            hidden_outputs, final_outputs = self.forwardPass(X)\n",
    "\n",
    "            # TODO: Output error - Replace this value with your calculations.\n",
    "            error = XXX # Output layer error is the difference between desired target and actual output.\n",
    "\n",
    "            ### Backward pass ###\n",
    "            delta_weights_i_h, delta_weights_h_o = self.backwardPass(X, delta_weights_i_h, delta_weights_h_o, hidden_outputs, error)\n",
    "            \n",
    "\n",
    "        # Update the weights - Replace these values with your calculations.\n",
    "        self.weights_hidden_to_output +=  delta_weights_h_o / n_records # update hidden-to-output weights with gradient descent step\n",
    "        self.weights_input_to_hidden +=  delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gccgIsAbgKwB"
   },
   "source": [
    "#◢ Tests unitaires\n",
    "\n",
    "Run these unit tests to check the correctness of your network implementation. This will help you be sure your network was implemented correctly befor you starting trying to train it. These tests must all be successful to pass the project.\n",
    "\n",
    "Lancez ces tests unitaires pour vérifier l'exactitude du fonctionnement de votre réseau. Cela vous permettra de vous assurer que votre réseau a été correctement implémenter avant de commencer à l'entrainer. \n",
    "\n",
    "Bien entendu, tous ces tests doivent tous être réussis !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgh1x98CgKwC"
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "inputs = np.array([[0.5, -0.2, 0.1]])\n",
    "targets = np.array([[0.4]])\n",
    "test_w_i_h = np.array([[0.1, -0.2],\n",
    "                       [0.4, 0.5],\n",
    "                       [-0.3, 0.2]])\n",
    "test_w_h_o = np.array([[0.3],\n",
    "                       [-0.1]])\n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for data loading\n",
    "    ##########\n",
    "    \n",
    "    def test_data_path(self):\n",
    "        # Test that file path to dataset has been unaltered\n",
    "        self.assertTrue(data_path.lower() == exercice1_path.lower() + '/bike-sharing-dataset/hour.csv')\n",
    "        \n",
    "    def test_data_loaded(self):\n",
    "        # Test that data frame loaded\n",
    "        self.assertTrue(isinstance(rides, pd.DataFrame))\n",
    "    \n",
    "    ##########\n",
    "    # Unit tests for network functionality\n",
    "    ##########\n",
    "\n",
    "    def test_activation(self):\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        # Test that the activation function is a sigmoid\n",
    "        self.assertTrue(np.all(network.activation_function(0.5) == 1/(1+np.exp(-0.5))))\n",
    "\n",
    "    def test_train(self):\n",
    "        # Test that weights are updated correctly on training\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        \n",
    "        network.train(inputs, targets)\n",
    "        self.assertTrue(np.allclose(network.weights_hidden_to_output, \n",
    "                                    np.array([[ 0.37275328], \n",
    "                                              [-0.03172939]])))\n",
    "        self.assertTrue(np.allclose(network.weights_input_to_hidden,\n",
    "                                    np.array([[ 0.10562014, -0.20185996], \n",
    "                                              [0.39775194, 0.50074398], \n",
    "                                              [-0.29887597, 0.19962801]])))\n",
    "\n",
    "    def test_forwardPass(self):\n",
    "        # Test correctness of forwardPass method\n",
    "        network = NeuralNetwork(3, 2, 1, 0.5)\n",
    "        network.weights_input_to_hidden = test_w_i_h.copy()\n",
    "        network.weights_hidden_to_output = test_w_h_o.copy()\n",
    "        hidden_outputs, final_outputs = network.forwardPass(inputs)\n",
    "\n",
    "        self.assertTrue(np.allclose(final_outputs, 0.09998924))\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestMethods())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55So0dhwgKwF"
   },
   "source": [
    "#◢ Entrainement du réseau\n",
    "\n",
    "Ici, vous allez définir les hyperparamètres du réseau. \n",
    "La stratégie consiste à trouver les bons hyperparamètres de telle sorte que l'erreur de prédiction sur le jeu d'entraînement soit la plus faible possible, en faisant attention de ne pas tomber dans l'overfitting. \n",
    "En effet, si vous entraînez le réseau trop longtemps ou si vous avez trop de neurones cachés, il peut devenir trop spécifique au jeu d'entraînement et ne pourra pas généraliser son apprentissage à d'autres données. Autrement dit, faites attention dès lors que l'erreur sur le jeu de validation commencera à augmenter à mesure que l'erreur sur le jeu de données d'entrainement diminuera.\n",
    "\n",
    "Vous utiliserez également une méthode appelée \"Stochastic Gradient Descent\" (SGD) pour entrainer le réseau. L'idée est que pour chaque itération, vous prenez un échantillon aléatoire des données au lieu d'utiliser l'ensemble des données. Vous effectuerez ainsi beaucoup plus d'itérations qu'avec une descente normale de gradient, mais chaque itération sera beaucoup plus rapide. Cela permet d'entraîner le réseau plus efficacement. Vous en saurez plus sur le SGD plus tard.\n",
    "\n",
    "### Choisissez le nombre d'itérations (epoch)\n",
    "Cycle d'apprentissage complet sur l'intégralité de l'ensemble de données de manière à ce que chaque exemple ait été vu une fois. Une itération représente ainsi N/taille du lot itérations d'apprentissage (batch), où N est le nombre total d'exemples. Plus vous utiliserez d'itérations, plus le modèle sera adapté aux données. Cependant, si vous utilisez trop d'itérations, alors le modèle ne s'adaptera pas bien aux autres données, c'est ce qu'on appelle le surajustement ou overfitting.\n",
    "\n",
    "### Choisissez le taux d'apprentissage (learning rate)\n",
    "Grandeur scalaire utilisée pour entraîner le modèle via la descente de gradient. À chaque itération, l'algorithme de descente de gradient multiplie le taux d'apprentissage par le gradient. Le produit ainsi généré est appelé pas de gradient.\n",
    "Si le pas est trop grand, les poids ont tendance à exploser et le réseau ne parvient pas à ajuster les données. Un bon choix pour commencer est de commencer à 0,1. Si le réseau a des difficultés à ajuster les données, essayez de réduire le taux d'apprentissage. Notez que plus le taux d'apprentissage est faible, plus les étapes de la mise à jour des poids sont petites et plus le réseau neuronal met du temps à converger.\n",
    "\n",
    "### Choisissez le nombre de neurones cachés\n",
    "Plus vous avez de neurones cachés, plus les prédictions du modèle seront précises. Essayez quelques chiffres différents et voyez comment cela affecte la performance. Vous pouvez consulter le dictionnaire des pertes pour obtenir une mesure de la performance du réseau. Si le nombre de neurones cachées est trop faible, le modèle n'aura pas assez d'espace pour apprendre et s'il est trop élevé, il y a trop d'options quant à la direction que peut prendre l'apprentissage. L'astuce consiste ici à trouver le bon équilibre dans le nombre de neurones cachés que vous choisissez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wrMuAaMlgKwF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "### Set the hyperparameters here ###\n",
    "iterations = 1000\n",
    "learning_rate = 0.8\n",
    "hidden_nodes = 10\n",
    "output_nodes = 1\n",
    "\n",
    "def MSE(y, Y):\n",
    "    return np.mean((y-Y)**2)\n",
    "\n",
    "N_i = train_features.shape[1]\n",
    "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
    "\n",
    "losses = {'train':[], 'validation':[]}\n",
    "for ii in range(iterations):\n",
    "    # Go through a random batch of 128 records from the training data set\n",
    "    batch = np.random.choice(train_features.index, size=128)\n",
    "    X, y = train_features.iloc[batch].values, train_targets.iloc[batch]['cnt']\n",
    "                             \n",
    "    network.train(X, y)\n",
    "    \n",
    "    # Printing out the training progress\n",
    "    train_loss = MSE(network.forwardPass(train_features)[1].T, train_targets['cnt'].values)\n",
    "    val_loss = MSE(network.forwardPass(val_features)[1].T, val_targets['cnt'].values)\n",
    "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
    "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
    "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    losses['train'].append(train_loss)\n",
    "    losses['validation'].append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_-GiwIAgKwH"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(losses['train'], label='Training loss')\n",
    "plt.plot(losses['validation'], label='Validation loss')\n",
    "plt.legend()\n",
    "_ = plt.ylim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MicDMLHgKwJ"
   },
   "source": [
    "#◢ Vérification de vos prédictions\n",
    "\n",
    "Ici, utilisez les données de test pour voir dans quelle mesure votre réseau est performant. \n",
    "Si quelque chose ne va pas, assurez-vous que chaque étape de votre réseau est correctement mise en œuvre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sTsNyvzHgKwK"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "mean, std = scaled_features['cnt']\n",
    "predictions = network.forwardPass(test_features)[1].T*std + mean\n",
    "ax.plot(predictions[0], label='Prediction')\n",
    "ax.plot((test_targets['cnt']*std + mean).values, label='Data')\n",
    "ax.set_xlim(right=len(predictions))\n",
    "ax.legend()\n",
    "\n",
    "dates = pd.to_datetime(rides.iloc[test_data.index]['dteday'])\n",
    "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
    "ax.set_xticks(np.arange(len(dates))[12::24])\n",
    "_ = ax.set_xticklabels(dates[12::24], rotation=45)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Premier_reseau_de_neurones.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
